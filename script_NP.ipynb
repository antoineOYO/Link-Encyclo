{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**goal is to trim 15 510 geographical articles, to charaterize each headphrase by the Geographical Entities (GE) mentionned in the article body**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/antoine/Documents/GitHub/linkencyclo')\n",
    "datapath = '/home/antoine/Documents/GitHub/datas/'\n",
    "\n",
    "from EncycloObject import Article, Book\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a `Book` instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book with 15510 articles\n",
      "plain version of the geo-articles, with the following attributes\n",
      "['volume', 'numero', 'headword', 'authors', 'text', 'hash', 'artfl', 'gold_coords', 'enccre', 'article_id', 'gold_qid']\n"
     ]
    }
   ],
   "source": [
    "with open(datapath+'geobook_22082024.pkl', 'rb') as f:\n",
    "    mybook = pickle.load(f)\n",
    "print(mybook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book with 15510\n"
     ]
    }
   ],
   "source": [
    "with open(datapath+'geobook_plain_22082024.pkl', 'rb') as f:\n",
    "    plain_geobook = pickle.load(f)\n",
    "print(plain_geobook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for article in mybook:\n",
    "#     head = article.headword\n",
    "#     article.headphrase = head\n",
    "#     del article.headword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath+'geobook_22082024.pkl', 'wb') as f:\n",
    "    pickle.dump(mybook, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the Article instance with hash `7/2925/GRENOBLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://artflsrv04.uchicago.edu/philologic4.7/encyclopedie0922/navigate/7/2925',\n",
       " '7/2925/GRENOBLE',\n",
       " 'GRENOBLE, Gratianopolis, (Géogr.) ancienne ville de France, capitale du Dauphiné, avec un évêché suffragant de Vienne, & un parlement érigé en 1493 par Louis XI. qui n\\'étoit encore que dauphin ;  mais son pere ratifia cette érection deux ans après.\\nGrenoble est sur l\\'Isere, à onze lieues S O. de Chambéri, quarante-deux N. O. de Turin, seize S. E. de Vienne, cent vingt-quatre S. O. de Paris. Long. suivant Harris, 23d. 31\\'. 15\". suivant Cassini, 23d. 14\\'. 15\". latit 45d. 11\\'.\\nCette ville reçut le nom de Gratianopolis de l\\'empereur  Gratien fils de Valentinien I. car elle s\\'appelloit  auparavant Cularo ; & c\\'est sous ce nom qu\\'il en est parlé dans une lettre de Plancus à Cicéron, epist. xxiij. Long-tems après, les Romains l\\'érigerent en cité: dans le cinquieme siecle, elle fut assujettie par les Bourguignons, & dans le sixieme par les François Mérovingiens ; ensuite elle a obéi à Lothaire, à Boson, à Charles le Gros, à Louis l\\'Aveugle, à Rodolphe II. à Conrad & à Rodolphe le lâche, ses fils, qui lui donnerent de grands priviléges.\\nOn met au nombre des jurisconsultes dont Grenoble est la patrie, Pape (Guy), qui mourut en 1487 ; son recueil de décisions des plus belles questions de droit, n\\'est pas encore tombé dans l\\'oubli.\\nM. de Bouchenu de Valbonnais, (Jean Pierre Moret) premier président du parlement de Grenoble, né dans cette ville le 23 Juin 1651, mérite le titre du plus savant historiographe de son pays, par la belle histoire du Dauphiné, qu\\'il a publiée en trois vol. in fol. il est mort en 1730, âgé de 79 ans. Il voyagea dans sa jeunesse, & se trouva sur la flotte d\\'Angleterre à la bataille de Solbaye, la plus furieuse qu\\'eût encore  vû Ruyter, & où l\\'on s\\'attribua l\\'avantage de part & d\\'autre. (D. J.)')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grenoble = mybook._reach_article(headphrase='grenoble')\n",
    "grenoble.artfl, grenoble.hash, grenoble.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['volume', 'numero', 'authors', 'text', 'hash', 'artfl', 'gold_coords', 'enccre', 'article_id', 'gold_qid', 'parsed', 'ner', 'headphrase'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grenoble.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech parsing\n",
    "Stanza doc are stored in the `.parsed` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 10:58:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f7ee720a5b42999e162a59bef56cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 10:58:49 INFO: Downloaded file to /home/antoine/stanza_resources/resources.json\n",
      "2024-08-22 10:58:49 INFO: Loading these models for language: fr (French):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2024-08-22 10:58:49 INFO: Using device: cpu\n",
      "2024-08-22 10:58:49 INFO: Loading: tokenize\n",
      "2024-08-22 10:58:50 INFO: Loading: mwt\n",
      "2024-08-22 10:58:50 INFO: Loading: pos\n",
      "2024-08-22 10:58:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlpstanza = stanza.Pipeline(lang='fr', processors='tokenize,mwt, pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in plain_geobook:\n",
    "    article.parsed = nlpstanza(article.text)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "We apply the BERT model to identify the GE mentionned in each article-body.\n",
    "\n",
    "From\n",
    "> 7/2925/GRENOBLE \n",
    "> \n",
    "> 'GRENOBLE, Gratianopolis, (Géogr.) ancienne ville de France, capitale du Dauphiné, avec un évêché suffragant de Vienne, ...'\n",
    "\n",
    "We want to store in the `article.ner` attribute the NER tags like :\n",
    "```python\n",
    "[\n",
    "    {'entity_group': 'NC_Spatial', 'score': 0.9601506, 'word': 'ancienne ville', 'start': 26, 'end': 40},\n",
    "    {'entity_group': 'NP_Spatial', 'score': 0.96639144, 'word': 'France', 'start': 44, 'end': 50},\n",
    "    {'entity_group': 'NC_Spatial', 'score': 0.96316826, 'word': 'capitale', 'start': 52, 'end': 60},\n",
    "    {'entity_group': 'NP_Spatial', 'score': 0.9662918, 'word': 'Dauphiné', 'start': 64, 'end': 72},\n",
    "    ...\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/antoine/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "bert = pipeline(\"token-classification\", model=\"GEODE/bert-base-french-cased-edda-ner\",\n",
    "                aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in plain_geobook:\n",
    "    article.ner = article._apply_pipeline(bert)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add NER tags to the Stanza doc thanks to the method `Article._enrich_stanzadoc`\n",
    "- native `Token` instances of the Stanza doc receive the NER tags\n",
    "- native `Span` instances of the Stanza doc receive the contiguous merged NER tags\n",
    "\n",
    "The GE phrases are normalized :\n",
    "- `ancienne ville` to `ville`\n",
    "- `royaume de France` to `france`\n",
    "- `St. Etienne` to `saint etienne`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stopwords from Solr\n",
    "with open('solr_stopwords.txt', 'r') as f:\n",
    "    stopwords = f.read().split('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABROLHOS\n"
     ]
    }
   ],
   "source": [
    "for article in mybook:\n",
    "    article._enrich_stanzadoc(stopwords=stopwords)\n",
    "    try :\n",
    "        ncs, nps = article._get_spatial_info(stopwords=stopwords)\n",
    "    except:\n",
    "        print(article.headphrase)\n",
    "        e = mybook._reach_article(headphrase=article.headphrase)\n",
    "        break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from stanza.models.common.doc import Span, Token\n",
    "\n",
    "VOC_SAINT = f\"sainte|saint|sant|san|st|s\"\n",
    "def is_saint(phrase):\n",
    "    \"\"\"\n",
    "    Returns True if the phrase contains a token in the VOC_SAINT list\n",
    "    \"\"\"\n",
    "    return bool(re.search(rf'\\b({VOC_SAINT})\\b', phrase))\n",
    "\n",
    "\n",
    "def normalize_span(span, pos=['NOUN', 'PROPN', 'ADJ'], stop_words=None):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    - span : Stanza Span instances (stanza.models.common.doc.Span)\n",
    "    \n",
    "    Normalizes Proper Nouns Phrases or Common Nouns Phrases by applying the following rules:\n",
    "\n",
    "    - Proper Nouns Phrase : set pos=[NOUN, 'PROPN', 'ADJ'] to remove tokens diffrent [NOUN, 'PROPN', 'ADJ']\n",
    "    - Common Nouns Phrase : set pos=[NOUN, 'PROPN'] to remove tokens diffrent [NOUN, 'PROPN']\n",
    "        - for CNP we also add 'PROPN' because some nouns ('Bourg', 'Isle', 'Ville') are tagged as 'PROPN'\n",
    "        because they are capitalized\n",
    "        - other issue : for 'isle' we suspect a out-of-vocabulary issue :\n",
    "        {\"text\": \"isle\", \"upos\": \"X\",}\n",
    "    - detect if the phrase is a saint name and prepend 'saint' in that case        \n",
    "    - apply lower(), unidecode() to each token\n",
    "    - remove tokens in a list of stopwords to double-check\n",
    "    - remove symbols and digits\n",
    "\n",
    "    Output :\n",
    "    list of Stanza Span instances with new attribute .norm_text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    norm_text = []\n",
    "\n",
    "    # if pattern for VOC_SAINT is found, we prepend 'saint'\n",
    "    if is_saint(unidecode(span.text.lower())):\n",
    "        norm_text = ['saint']\n",
    "    \n",
    "    # keeping only tokens with the right POS\n",
    "    norm_text.extend(\n",
    "        [\n",
    "            unidecode(word.text.lower()) for word in span.words \\\n",
    "                if word.upos in pos \\\n",
    "                    and not is_saint(unidecode(word.text.lower()))                            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # remove stopwords\n",
    "    norm_text = [token for token in norm_text if token not in stop_words]\n",
    "\n",
    "    # remove symbols and digits\n",
    "    norm_text = [re.sub(r'[^a-z\\s]', '', token) for token in norm_text]\n",
    "\n",
    "    norm_text = ' '.join(norm_text)\n",
    "    \n",
    "    # adding the new attribute\n",
    "    span.norm_text = norm_text\n",
    "    #norm_nps.append(np)\n",
    "    if not hasattr(span, 'norm_text'):\n",
    "        print(span.text)\n",
    "    return span\n",
    "\n",
    "# def normalize_NC(ncs, NC_pos=['NOUN', 'PROPN'], stop_words=None):\n",
    "#     \"\"\"\n",
    "#     Input :\n",
    "#     - ncs : list of Stanza Span instances (stanza.models.common.doc.Span)\n",
    "    \n",
    "#     Normalizes **Noms Communs** Phrases by applying the following rules:\n",
    "#     - remove tokens not tagged as [NOUN, 'PROPN'] = NC_pos.\n",
    "#         we also add 'PROPN' because some nouns ('Bourg', 'Isle', 'Ville') are tagged as 'PROPN'\n",
    "#         other issue : for 'isle' we suspect a out-of-vocabulary issue :\n",
    "#             # {\"text\": \"isle\", \"upos\": \"X\",}\n",
    "#     - apply lower(), unidecode() to each token\n",
    "#     - remove tokens in a list of stopwords to ensure\n",
    "#     - remove symbols and digits\n",
    "\n",
    "#     Output :\n",
    "#     list of Stanza Span instances with attribute .norm_text\n",
    "#     \"\"\"\n",
    "#     for np in ncs:\n",
    "#         norm_nps = []\n",
    "\n",
    "#         # if pattern for VOC_SAINT is found, we delete it and prepend 'saint'\n",
    "#         if is_saint(unidecode(np.text.lower())):\n",
    "#             norm_nps = ['saint']\n",
    "#             continue\n",
    "        \n",
    "#         norm_nps.extend(\n",
    "#             [\n",
    "#                 unidecode(word.text.lower()) for word in np.words \\\n",
    "#                     if word.upos in NP_pos \\\n",
    "#                         and not is_saint(unidecode(word.text.lower()))                            \n",
    "#             ]\n",
    "#         )\n",
    "#         norm_nps = [token for token in norm_nps if token not in stop_words]\n",
    "\n",
    "#         norm_nps = [re.sub(r'[^a-z\\s]', '', token) for token in norm_nps]\n",
    "#         norm_nps = ' '.join(norm_nps)\n",
    "#         np.norm_text = norm_nps\n",
    "#         #norm_nps.append(np)\n",
    "#     return nps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = [entity_span for entity_span in e.parsed.entities if entity_span.type == 'NP_Spatial']\n",
    "nps = [normalize_span(span, pos=['NOUN', 'PROPN', 'ADJ'], stop_words=stopwords) for span in nps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sainte-Barbe saint\n",
      "Brésil bresil\n"
     ]
    }
   ],
   "source": [
    "for np in nps:\n",
    "    print(np.text, np.norm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Span' object has no attribute 'norm_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mnps:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(span\u001b[38;5;241m.\u001b[39mtext, span\u001b[38;5;241m.\u001b[39mnorm_text)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Span' object has no attribute 'norm_text'"
     ]
    }
   ],
   "source": [
    "for span in e.nps:\n",
    "    print(span.text, span.norm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute count of NP-Spatial\n",
    "(Geographic Entities that are NP with a proper Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
